{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Custom Fidelity-Based Optimizer:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Module pennylane has no attribute fidelity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 109\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Train using custom optimizer\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with Custom Fidelity-Based Optimizer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 109\u001b[0m final_params_custom, cost_history_custom, fidelity_history_custom \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_params_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfidelity_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\n\u001b[0;32m    114\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining with Adam Optimizer:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Train using Adam optimizer\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 48\u001b[0m, in \u001b[0;36mcustom_train\u001b[1;34m(initial_params, learning_rate, fidelity_threshold, max_iterations)\u001b[0m\n\u001b[0;32m     45\u001b[0m fidelity_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iterations):\n\u001b[1;32m---> 48\u001b[0m     current_cost \u001b[38;5;241m=\u001b[39m \u001b[43mcost_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     cost_history\u001b[38;5;241m.\u001b[39mappend(current_cost)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\qnode.py:842\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m         set_shots(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_device, override_shots)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_gradient_fn)()\n\u001b[0;32m    841\u001b[0m \u001b[38;5;66;03m# construct the tape\u001b[39;00m\n\u001b[1;32m--> 842\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    845\u001b[0m using_custom_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__setitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__delitem__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    849\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\qnode.py:751\u001b[0m, in \u001b[0;36mQNode.construct\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m old_interface \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface \u001b[38;5;241m=\u001b[39m qml\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mget_interface(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m--> 751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape \u001b[38;5;241m=\u001b[39m make_qscript(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39m_qfunc_output\n\u001b[0;32m    754\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtape\u001b[38;5;241m.\u001b[39mget_parameters(trainable_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\tape\\qscript.py:1371\u001b[0m, in \u001b[0;36mmake_qscript.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1370\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m AnnotatedQueue() \u001b[38;5;28;01mas\u001b[39;00m q:\n\u001b[1;32m-> 1371\u001b[0m         result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1373\u001b[0m     qscript \u001b[38;5;241m=\u001b[39m QuantumScript\u001b[38;5;241m.\u001b[39mfrom_queue(q)\n\u001b[0;32m   1374\u001b[0m     qscript\u001b[38;5;241m.\u001b[39m_qfunc_output \u001b[38;5;241m=\u001b[39m result\n",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m, in \u001b[0;36mcost_fn\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;129m@qml\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(dev)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost_fn\u001b[39m(params):\n\u001b[0;32m     19\u001b[0m     circuit(params, wires\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfidelity\u001b[49m(qml\u001b[38;5;241m.\u001b[39mStatePrep([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], wires\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]), wires\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pennylane\\__init__.py:358\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpennylane\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgrouping\u001b[39;00m  \u001b[38;5;66;03m# pylint:disable=import-outside-toplevel,consider-using-from-import\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grouping\n\u001b[1;32m--> 358\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Module pennylane has no attribute fidelity"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Setting Up the Quantum Device\n",
    "dev = qml.device('default.qubit', wires=2)\n",
    "\n",
    "# 2. Defining the QNN Ansatz\n",
    "def circuit(params, wires):\n",
    "    qml.RY(params[0], wires=0)\n",
    "    qml.RY(params[1], wires=1)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(params[2], wires=0)\n",
    "    qml.RY(params[3], wires=1)\n",
    "\n",
    "# 3. Defining the Cost Function\n",
    "@qml.qnode(dev)\n",
    "def cost_fn(params):\n",
    "    circuit(params, wires=[0, 1])\n",
    "    return 1 - qml.fidelity(qml.StatePrep([0, 0, 0, 1], wires=[0, 1]), wires=[0, 1])\n",
    "\n",
    "# 4. Combined Circuit for Fidelity Measurement\n",
    "@qml.qnode(dev)\n",
    "def fidelity_circuit(theta_old, theta_new):\n",
    "    # Apply the original parameters\n",
    "    circuit(theta_old, wires=[0, 1])\n",
    "    # Apply the adjoint (inverse) of the new parameters\n",
    "    qml.RY(-theta_new[3], wires=1)\n",
    "    qml.RY(-theta_new[2], wires=0)\n",
    "    qml.CNOT(wires=[0, 1])\n",
    "    qml.RY(-theta_new[1], wires=1)\n",
    "    qml.RY(-theta_new[0], wires=0)\n",
    "    return qml.probs(wires=[0, 1])\n",
    "\n",
    "# 5. Fidelity Calculation\n",
    "def compute_fidelity(theta_old, theta_new):\n",
    "    probs = fidelity_circuit(theta_old, theta_new)\n",
    "    fidelity = probs[0]  # Probability of |00>\n",
    "    return fidelity\n",
    "\n",
    "# 6. Custom Optimizer\n",
    "def custom_train(initial_params, learning_rate=0.1, fidelity_threshold=0.99, max_iterations=100):\n",
    "    params = initial_params.copy()\n",
    "    cost_history = []\n",
    "    fidelity_history = []\n",
    "    \n",
    "    for it in range(max_iterations):\n",
    "        current_cost = cost_fn(params)\n",
    "        cost_history.append(current_cost)\n",
    "        \n",
    "        # Compute gradients\n",
    "        grads = qml.grad(cost_fn)(params)\n",
    "        \n",
    "        # Propose new parameters\n",
    "        params_new = params - learning_rate * grads\n",
    "        \n",
    "        # Compute fidelity\n",
    "        fidelity = compute_fidelity(params, params_new)\n",
    "        fidelity_history.append(fidelity)\n",
    "        \n",
    "        # Decide to accept or reject\n",
    "        if fidelity >= fidelity_threshold:\n",
    "            params = params_new\n",
    "            print(f\"Iteration {it+1}: Cost = {current_cost:.6f}, Fidelity = {fidelity:.6f} -> Update Accepted\")\n",
    "        else:\n",
    "            # Reduce learning rate and do not update\n",
    "            learning_rate *= 0.5\n",
    "            print(f\"Iteration {it+1}: Cost = {current_cost:.6f}, Fidelity = {fidelity:.6f} -> Update Rejected, New Learning Rate = {learning_rate}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if current_cost < 1e-6:\n",
    "            print(\"Convergence achieved.\")\n",
    "            break\n",
    "    \n",
    "    return params, cost_history, fidelity_history\n",
    "\n",
    "# 7. Built-In Adam Optimizer Training\n",
    "def adam_train(initial_params, max_iterations=100):\n",
    "    opt = qml.AdamOptimizer(stepsize=0.1)\n",
    "    params = initial_params.copy()\n",
    "    cost_history = []\n",
    "    \n",
    "    for it in range(max_iterations):\n",
    "        current_cost = cost_fn(params)\n",
    "        cost_history.append(current_cost)\n",
    "        \n",
    "        # Update parameters\n",
    "        params, _ = opt.step_and_cost(cost_fn, params)\n",
    "        \n",
    "        print(f\"Iteration {it+1}: Cost = {current_cost:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if current_cost < 1e-6:\n",
    "            print(\"Convergence achieved.\")\n",
    "            break\n",
    "    \n",
    "    return params, cost_history\n",
    "\n",
    "# 8. Running the Training\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize parameters\n",
    "initial_params_custom = np.random.uniform(0, 2*np.pi, 4)\n",
    "initial_params_adam = initial_params_custom.copy()\n",
    "\n",
    "# Train using custom optimizer\n",
    "print(\"Training with Custom Fidelity-Based Optimizer:\")\n",
    "final_params_custom, cost_history_custom, fidelity_history_custom = custom_train(\n",
    "    initial_params=initial_params_custom,\n",
    "    learning_rate=0.1,\n",
    "    fidelity_threshold=0.99,\n",
    "    max_iterations=50\n",
    ")\n",
    "\n",
    "print(\"\\nTraining with Adam Optimizer:\")\n",
    "# Train using Adam optimizer\n",
    "final_params_adam, cost_history_adam = adam_train(\n",
    "    initial_params=initial_params_adam,\n",
    "    max_iterations=50\n",
    ")\n",
    "\n",
    "# 9. Plotting the Results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Cost Function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(cost_history_custom, label='Custom Optimizer')\n",
    "plt.plot(cost_history_adam, label='Adam Optimizer')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost Function Convergence')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Fidelity\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fidelity_history_custom, label='Fidelity')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Fidelity')\n",
    "plt.title('Fidelity Between Parameter Updates')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
